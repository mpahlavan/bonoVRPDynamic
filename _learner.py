# -*- coding: utf-8 -*-
"""learner.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BJsJklWogKHTjoODFTQfYH2LvFb1Zaxh
"""

from marpdan.layers import TransformerEncoder, MultiHeadAttention

import torch
import torch.nn as nn
import torch.nn.functional as ‌F

class AttentionLearner(nn.Module):
    def __init__(self, cust_feat_size, veh_state_size, model_size = 128,
            layer_count = 3, head_count = 8, ff_size = 512, tanh_xplor = 10, greedy = False):
            #The init takes the key input sizes and model architecture hyperparamete
        r"""
        :param model_size:  Dimension :math:`D` shared by all intermediate layers
        :param layer_count: Number of layers in customers' (graph) Transformer Encoder
        :param head_count:  Number of heads in all Multi-Head Attention layers
        :param ff_size:     Dimension of the feed-forward sublayers in Transformer Encoder
        :param tanh_xplor:  Enable tanh exploration and set its amplitude
        """
        # By calling super().__init__(), it calls the __init__ method of the parent nn.Module class
        super().__init__()

        #it stores the model_size hyperparameter that was passed to the __init__ method.
        self.model_size = model_size
        #it calculates the inverse square root of the model size, also known as 1/sqrt(d), and assigns to inv_sqrt_d attribute.
        #This is a common operation in attention mechanisms to scale the dot product calculations between queries and key vectors.
        #By dividing by sqrt(d), it helps prevent very large values when multiplying high dimensional vectors.
        self.inv_sqrt_d = model_size ** -0.5

        # The tanh_xplor attribute is related to controlling the magnitude of the scores predicting which customer a vehicle should visit next.
        ##tanh ***
        self.tanh_xplor = tanh_xplor

        # Since the depot node has no features, its embedding layer should not depend on cust_feat_size.
        # Learnable depot embedding (claude)
        self.depot_embedding = nn.Linear(cust_feat_size, model_size)
        # is linear layers that map the features of the customers, to a common embedding space (model_size).
        #
        self.cust_embedding  = nn.Linear(cust_feat_size, model_size)
        # it defines a Transformer encoder for encoding the customer features. The encoder consists of several layers, each of which includes a multi-head attention mechanism and a feed-forward neural network.
        self.cust_encoder    = TransformerEncoder(layer_count, head_count, model_size, ff_size)

        # These lines define two multi-head attention mechanisms. One is used for attending to the fleet (set of vehicles),
        self.fleet_attention = MultiHeadAttention(head_count, veh_state_size, model_size)
        # it is used for attending to the individual vehicles.
        self.veh_attention   = MultiHeadAttention(head_count, model_size)
        # self.cust_project is a way to adjust the features learned by the customer encoder so that they can be effectively used with the vehicle features
        # in the attention mechanisms. It’s a common technique in neural networks to use linear layers for such transformations.
        # to provide the model with some extra flexibility when constructing the final customer representations that will be used for attention.
        self.cust_project    = nn.Linear(model_size, model_size)

        # Whether to select actions greedily
        #  If True, selects actions greedily; otherwise, samples from the action distribution.
        self.greedy = greedy

    #L_v: Number of vehicles in each sample in the batch. So vehicles has shape (N, L_v, D_v).
    #D_v: Size of vehicle state features
    #L_c: Number of customers
    #D_c: Size of customer features
    #N: Batch size
    #D: Hidden representation size / This is the embedding size, commonly 128 or 64.
      #model_size is a crucial hyperparameter that controls the dimensionality of the representations and the capacity of the Transformer Encoder.
      #layer_count, head_count, and ff_size are additional hyperparameters that influence the Transformer Encoder's architecture and complexity.

    def _encode_customers(self, customers, mask = None):
        r"""
        :param customers: :math:`N \times L_c \times D_c` tensor containing minibatch of customers' features
        :param mask:      :math:`N \times L_c` tensor containing minibatch of masks
                where :math:`m_{nj} = 1` if customer :math:`j` in sample :math:`n` is hidden (pad or dyn), 0 otherwise
        """
        #self.depot_embedding: Linear layer for embedding the features of the depot (the first customer in the sequence).
        #self.cust_embedding: Linear layer for embedding the features of the remaining customers in the sequence.
        #The embeddings are concatenated along the second dimension (dim=1) to form the tensor cust_emb.
        cust_emb = torch.cat((
            self.depot_embedding(customers[:,0:1,:]),
            self.cust_embedding(customers[:,1:,:])
            ), dim = 1) #.size() = N x L_c x D
                        #The resulting tensor has shape (N, L_c, D) where N is the batch size, L_c is the number of customers, and D is the model size.

        #If a mask is provided (indicating hidden or masked customers), this step sets the embeddings of those customers to zero.
        if mask is not None:
            cust_emb[mask] = 0

        #The customer embeddings (cust_emb) are passed through a Transformer Encoder (self.cust_encoder) to capture dependencies and relationships among customers.
        self.cust_enc = self.cust_encoder(cust_emb, mask) #.size() = N x L_c x D
        # Precomputation for the fleet attention mechanism, preparing it to attend to the encoded customer features.
        self.fleet_attention.precompute(self.cust_enc)
        # The encoded customer features are projected using a linear layer (self.cust_project) to obtain the final customer representations.
        self.cust_repr = self.cust_project(self.cust_enc) #.size() = N x L_c x D
        #If a mask is provided, this step sets the representations of masked customers to zero.
        if mask is not None:
            self.cust_repr[mask] = 0

    # method _encode_customers:this method processes the input customer features through embeddings, applies masking, utilizes a Transformer Encoder, precomputes attention for fleet attention, and projects the final customer representations.
    #This is a crucial step in preparing the customer features for further processing in the model.


    def _repr_vehicle(self, vehicles, veh_idx, mask):
      #Generates a representation for the currently acting vehicle, considering its context within the fleet and incorporating vehicle constraints.
        r"""
        :param vehicles: :math:`N \times L_v \times D_v` tensor containing minibatch of vehicles' states
        :param veh_idx:  :math:`N \times 1` tensor containing minibatch of indices corresponding to currently acting vehicle
        :param mask:     :math:`N \times L_v \times L_c` tensor containing minibatch of masks
                where :math:`m_{nij} = 1` if vehicle :math:`i` cannot serve customer :math:`j` in sample :math:`n`, 0 otherwise

        :return:         :math:`N \times 1 \times D` tensor containing minibatch of representations for currently acting vehicle
        """
        # "This line uses the MultiHeadAttention layer defined as self.fleet_attention to compute representations for each vehicle in the fleet.
        #vehicles has shape (N, L_v, D_v) containing the state features of each vehicle in the batch.
        #mask has shape (N, L_v, L_c) indicating which vehicles can serve which customers.
        #We pass vehicles as the query and vehicles again as both the keys and values to the MultiHeadAttention layer. This allows each vehicle to attend to all other vehicles bidirectionally.
        #The attention is scaled by 1/sqrt(D) to counteract the dot product magnitude increasing with dimensionality.
        #MultiHeadAttention splits the computation into multiple heads to jointly attend to different subspaces.The output containing the attended representation of each vehicle."


        #The resulting tensor (fleet_repr) has shape (N, L_v, D) where N is the batch size, L_v is the number of vehicles, and D is the model size.
        fleet_repr = self.fleet_attention(vehicles, mask = mask) #.size() = N x L_v x D
        # The representation of the currently acting vehicle is obtained by gathering from the fleet representation based on the vehicle index (veh_idx).veh_idx has shape (N, 1)
        #
        # veh_idx.unsqueeze(2).expand(-1, -1, self.model_size) shapes the index for gathering.
        veh_query = fleet_repr.gather(1, veh_idx.unsqueeze(2).expand(-1, -1, self.model_size)) #.size() = N x 1 x D
        #The method applies vehicle attention (self.veh_attention) using the vehicle query, fleet representation as both keys and values.
        return self.veh_attention(veh_query, fleet_repr, fleet_repr) #.size() = N x 1 x D


    def _score_customers(self, veh_repr):
        r"""
        :param veh_repr: :math:`N \times 1 \times D` tensor containing minibatch of representations for currently acting vehicle

        :return:         :math:`N \times 1 \times L_c` tensor containing minibatch of compatibility scores between currently acting vehicle and each customer
        """
        # how well a given customer is compatible or suitable for being served by the currently acting vehicle. The compatibility scores are computed based on the representations of the vehicle and the customers.
        #The goal is to compute compatibility scores between a vehicle representation and all customer representations.
        # veh_repr = ... # N x 1 x D /// self.cust_repr = ... # N x L_c x D //// First, a matrix multiplication is done between veh_repr and cust_repr.transpose(1, 2)
        #This transforms them into shapes: veh_repr: N x 1 x D ///cust_repr: N x D x L_c
        # Giving a score between the vehicle and each customer.
        compat = veh_repr.matmul( self.cust_repr.transpose(1, 2) ) #.size() = N x 1 x L_c
        # Scales the scores by inverse square root of representation size. Common in dot-product attention.
        compat *= self.inv_sqrt_d
        # Applies tanh exploration to compat scores if enabled. Amplitude set by hyperparam.
        # If tanh exploration is enabled (i.e., self.tanh_xplor is not None), the compatibility scores undergo a hyperbolic tangent (tanh) transformation.
        #The tanh exploration can introduce non-linearity and adjust the exploration-exploitation behavior of the model. The self.tanh_xplor parameter determines the amplitude of this tanh exploration.
        if self.tanh_xplor is not None:
            compat = self.tanh_xplor * compat.tanh()
        # The compatibility scores are crucial in the decision-making process of selecting a customer for the currently acting vehicle. Higher compatibility scores indicate a better match,
        #and these scores are often used to compute probabilities or rankings for choosing the next customer to be served. The exploration-exploitation balance can be adjusted by scaling and applying non-linear transformations to the compatibility scores, influencing the decision-making strategy of the model.
        return compat


    def _get_logp(self, compat, veh_mask):
      #The _get_logp method computes log-probabilities for choosing which customer to serve next based on the compatibility scores and vehicle masks.
        r"""
        :param compat:   :math:`N \times 1 \times L_c` tensor containing minibatch of compatibility scores between currently acting vehicle and each customer
        :param veh_mask: :math:`N \times 1 \times L_c` tensor containing minibatch of masks
                where :math:`m_{nj} = 1` if currently acting vehicle cannot serve customer :math:`j` in sample :math:`n`, 0 otherwise

        :return:         :math:`N \times L_c` tensor containing minibatch of log-probabilities for choosing which customer to serve next
        """
        #The compatibility scores (compat) are adjusted by setting the scores corresponding to incompatible customers (where veh_mask is 1) to negative infinity (-float('inf')).
        #This masking operation ensures that incompatible customers are not considered during the probability computation.
        compat[veh_mask] = -float('inf')
        #Log-Softmax Transformation:The masked compatibility scores are then passed through a log-softmax transformation along the last dimension (dimension 2). This transformation converts the compatibility scores into log-probabilities.
        #The log-softmax function is often used in probability distributions to ensure numerical stability when dealing with small or large values.
        # Squeezing Dimensions:The result is a tensor of log-probabilities for each customer in the minibatch. The dimensions are adjusted to remove the singleton dimension introduced by the compatibility scores (compat) along dimension 1 (using squeeze(1)).
        #The final output is a tensor containing log-probabilities for each customer in the minibatch, indicating the likelihood of each customer being chosen next by the currently acting vehicle.
        return compat.log_softmax(dim = 2).squeeze(1)
        #This log-probability tensor is typically used during the training process for computing the loss and updating the model parameters. During inference or generation, these log-probabilities can be used to sample or select the next customer based on a probabilistic strategy.



    def step(self, dyna):
      #The step method in the AttentionLearner class is a key component of the reinforcement learning process in the context of customer service selection.

        #_repr_vehicle method is called with the current state of the vehicles (dyna.vehicles), the index of the currently acting vehicle(dyna.cur_veh_idx),
        # and the mask indicating which customers can be served by the current vehicle (dyna.mask).
        veh_repr = self._repr_vehicle(dyna.vehicles, dyna.cur_veh_idx, dyna.mask)
        #_score_customers method is called with the obtained vehicle representation (veh_repr).
        #This computes compatibility scores (compat) between the currently acting vehicle and each customer.
        compat = self._score_customers(veh_repr)
        #_get_logp method is used to get log-probabilities (logp) for choosing each customer next, considering the compatibility scores and vehicle masks.
        logp = self._get_logp(compat, dyna.cur_veh_mask)
        #If the greedy flag is set to True, the customer index (cust_idx) with the highest log-probability is chosen using the argmax operation along dimension 1.
        #If greedy is False, a customer index (cust_idx) is sampled from the multinomial distribution defined by the exponentiated log-probabilities.
        if self.greedy:
            cust_idx = logp.argmax(dim = 1, keepdim = True)
        else:
            cust_idx = logp.exp().multinomial(1)
        #The selected customer index (cust_idx) and the log-probability associated with the selected customer are returned as the output of the step method.
        return cust_idx, logp.gather(1, cust_idx)
    #In summary, the step method captures the decision-making process of the attention-based model for selecting the next customer to be served by the currently acting vehicle. The choice is based on compatibility scores and log-probabilities, and it can follow either a greedy or non-greedy strategy depending on the value of the greedy flag.

    def forward(self, dyna):
      #The forward method in the AttentionLearner class orchestrates the interaction of the attention-based model with the environment represented by the dyna object.

        #The dyna.reset() method is called to reset the environment's state, preparing for a new episode of interaction.
        dyna.reset()
        #Three lists (actions, logps, rewards) are initialized to record information during the interaction.
        actions, logps, rewards = [], [], []
        #A while loop is used to interact with the environment until it is marked as "done."
        #If there are new customers (dyna.new_customers is True), the customers are encoded using the _encode_customers method.
        while not dyna.done:
            if dyna.new_customers:
                self._encode_customers(dyna.nodes, dyna.cust_mask)
            #he step method is called to choose the next customer to be served, and the resulting customer index (cust_idx) and log-probability (logp) are recorded.
            cust_idx, logp = self.step(dyna)
            #The selected action (vehicle index and customer index), log-probability, and the reward obtained from taking the chosen action are appended to the corresponding lists (actions, logps, rewards).
            actions.append( (dyna.cur_veh_idx, cust_idx) )
            logps.append( logp )
            rewards.append( dyna.step(cust_idx) )
        #Once the interaction is complete (the environment is marked as "done"), the lists containing recorded information are returned as the output of the forward method.
        return actions, logps, rewards
    #In summary, the forward method encapsulates the entire interaction process between the attention-based model and the environment for a single episode.
    # It records relevant information at each step and returns the collected data, which can be used for training the model through reinforcement learning.




    #***The tanh_xplor hyperparameter in the AttentionLearner model adds an exploration bonus to the action scores during training. Here is a more detailed explanation:
    #The action scores (compat) indicate how compatible each customer is for the current vehicle. Higher values mean better actions.
    #During training, the model may sometimes assign low values to good actions, and get stuck in local optima. This is known as the "underestimation" problem in reinforcement learning.
    #To counter this, we add an exploration bonus proportional to tanh(compat) with amplitude tanh_xplor.
    #For low action values, the tanh bonus is high, which encourages exploration of those actions.
    #For high values, the tanh bonus is near 0, so it does not distort the score much.
    #So tanh_xplor controls how much we want to boost the exploration. Higher values encourage more exploration.
    #In vehicle routing specifically:

    #The agent may not know initially which customers are good to pair up.
    #The tanh bonus helps it explore seemingly bad actions, and discover good customer clusters.
    #This prevents it getting stuck routing the same short loops.
    #In summary, the tanh exploration bonus counters action value underestimation and encourages more diverse exploration during training. This helps the routing agent discover solutions it may otherwise never explore.






#Constructor:

#Embeddings and encoder to get customer representations
#Attention layers to summarize vehicle state and customer representations
#Projection layers to transform representations
#Exploration parameters like tanh scaling

#Methods:

#_encode_customers():
#Gets depot + customer embeddings
#Encodes customer features using transformer
#Projects to get final representations

#_repr_vehicle():
#Attends over vehicles to get individual vehicle representation

#_score_customers():
#Scores compatibility with current vehicle

#_get_logp():
#Converts scores to log probabilities

#step():
#Gets vehicle representation
#Scores customers
#Samples the next customer

#forward():
#Resets environment
#Encodes customers
#epeatedly samples actions using step()
#Returns (actions, log ps, rewards) tuple

#So in summary, it uses attention over vehicle states and customer embeddings to learn a policy for routing vehicles dynamically.


#****************************************************************
#This code defines a class AttentionLearner which is a subclass of PyTorch’s nn.Module. The class is designed to learn from a vehicle routing problem with time windows (VRPTW) using attention mechanisms. Here’s a breakdown of the key components:

#__init__: This is the constructor for the AttentionLearner class. It initializes various parameters and layers used in the model. These include the size of the model (model_size), the number of layers in the Transformer Encoder (layer_count), the number of heads in all Multi-Head Attention layers (head_count), the dimension of the feed-forward sublayers in the Transformer Encoder (ff_size), and a parameter for tanh exploration (tanh_xplor).

#_encode_customers: This method encodes the customers’ features. It takes as input a tensor of customers’ features and an optional mask tensor. It first applies a linear transformation to the customers’ features, then passes them through a Transformer Encoder. The encoded features are then projected to the model size dimension.

#_repr_vehicle: This method generates a representation for the currently acting vehicle. It takes as input a tensor of vehicles’ states, a tensor of indices corresponding to the currently acting vehicle, and a mask tensor. It first computes a representation of the fleet of vehicles using multi-head attention, then selects the representation of the currently acting vehicle and refines it using another round of multi-head attention.

#_score_customers: This method computes compatibility scores between the currently acting vehicle and each customer. It takes as input a tensor representing the currently acting vehicle. It computes the dot product between the vehicle representation and the customer representations, scales it by the inverse square root of the model size, and applies a tanh function if tanh_xplor is not None.

#_get_logp: This method is not fully shown in the provided code, but it likely computes log probabilities based on the compatibility scores and a mask indicating which customers can be served by the vehicle.

#The code uses concepts from the Transformer model, such as self-attention and multi-head attention, to learn representations of the customers and vehicles that capture their interactions. This can be useful in problems like VRPTW where the relationships between entities (customers and vehicles) are important. The learned representations can then be used to make decisions, such as which customer a vehicle should serve next.
# The use of attention mechanisms allows the model to focus on different parts of the input depending on the current state, which can lead to more flexible and powerful models. The code also includes some exploration mechanism (tanh_xplor), which can be useful in reinforcement learning settings where the model needs to balance exploitation (making the best decision based on current knowledge) and exploration (trying out new decisions to gain more knowledge).
# The greedy attribute suggests that there might be a greedy decoding option in this model, which typically makes the decision that looks the best at the current step without considering future steps. However, without the full code, it’s hard to say exactly how these components are used in the overall model.