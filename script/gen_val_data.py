# -*- coding: utf-8 -*-
"""gen_val_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wnn9lbTsHy1naJyR96kTmntrpmAG-NQa
"""

from marpdan.problems import *
from marpdan.externals import lkh_solve, ort_solve
from marpdan.utils import eval_apriori_routes

import torch
from torch.utils.data import DataLoader
import pickle
import os

BATCH_SIZE = 10000
#It's a constant value used to initialize the random number generation process so that the results are reproducible.
#If you were to change the seed to a different value, you would get a different sequence of random numbers,but as long as the seed remains the same, the randomness is reproducible.
SEED = 231034871114
LATE_PS = [0.05, 0.1, 0.2, 0.3, 0.5]
ROLLOUTS = 100


#Sets the seed for PyTorch's random number generators (RNGs) to a specific value (SEED).
#Ensures reproducibility of results for operations involving randomness: initialization of weights, random sampling, data shuffling, etc.
torch.manual_seed(SEED)


# CVRP Data
# Iterates through three different problem sizes:    n: Number of customers to be visited. m: Number of available vehicles.
for n,m in ((10,2), (20,4), (50,10)):
    #Constructs a directory name dynamically using n and m values (e.g., "cvrp_n10m2").
    out_dir = "cvrp_n{}m{}".format(n, m)
    #Creates the specified directory if it doesn't exist. exist_ok=True prevents errors if the directory already exists.
    os.makedirs(out_dir, exist_ok = True)

    #VRPTW_Dataset.generate(...) Invokes a method named generate from a class named VRPTW_Dataset.This class likely specializes in creating instances of (VRPTW).
    #tw_ratio=0.0: Specifies no time windows for this particular generation (tw_ratio controls the strictness of time windows).
    #cust_dur_range=(0,1): Sets a range of customer service durations (likely between 0 and 1 time units).
    data = VRPTW_Dataset.generate(BATCH_SIZE, n, m, tw_ratio = 0.0, cust_dur_range = (0,1))

    #Calculates the range (maximum minus minimum) of the first two dimensions of the nodes attribute in the data object.
    x_scl = data.nodes[:,:,:2].max() - data.nodes[:,:,:2].min()
    #with open(..) Opens a file named "kool_data.pkl" in write-binary mode within the specified output directory.
    with open(os.path.join(out_dir, "kool_data.pkl"), 'wb') as f:
        #Saves a list of tuples, where each tuple contains:Normalized coordinates of the depot (scaled by x_scl).
        #Normalized coordinates of customer nodes (scaled by x_scl).Demand values of customer nodes (third dimension of data.nodes).
        #vehicle capacities for each batch instance (repeated for consistency).
        #zip(...): Combines the lists element-wise, creating tuples where each element of the tuple comes from a different list.
        #list(...): Converts the result of zip into a Python list.
        #pickle.dump(..., f, pickle.HIGHEST_PROTOCOL): Serializes and saves the list into the file object f using the highest protocol available in the pickle module.
        pickle.dump(list(zip(
            data.nodes[:,0,:2].div(x_scl).tolist(),
            data.nodes[:,1:,:2].div(x_scl).tolist(),
            data.nodes[:,1:,2].tolist(),
            [data.veh_capa for b in range(BATCH_SIZE)]
            )), f, pickle.HIGHEST_PROTOCOL)
            #data.nodes[:, 0, :2].div(x_scl).tolist(): Extracts the first node's (batch dimension, x, y) coordinates, divides them element-wise by x_scl, and converts the result to a Python list.
            #data.nodes[:, 1:, 2].tolist(): Extracts the demands of the remaining nodes and converts the result to a Python list.
            #[data.veh_capa for b in range(BATCH_SIZE)]: Creates a list with the vehicle capacity repeated BATCH_SIZE times.


    #Calls a method named normalize on the data object, likely a custom dataset object or a container with dedicated normalization functionality.
    #This method likely performs internal normalization of features within the data object, ensuring consistent scales for different attributes.
    data.normalize()
    #Creates a file named "norm_data.pyth" within the "data" subdirectory of the specified output directory.
    #Stores the entire data object, including its normalized features and any associated metadata.
    torch.save(data, os.path.join("data", out_dir, "norm_data.pyth"))

# CVRPTW Data
for n,m in ((10,2), (20,4), (50,10)):
    out_dir = "data/cvrptw_n{}m{}".format(n,m)
    os.makedirs(out_dir, exist_ok = True)

    data = VRPTW_Dataset.generate(BATCH_SIZE, n, m)

    data.normalize()
    torch.save(data, os.path.join(out_dir, "norm_data.pyth"))


# S-CVRPTW Data (more tw)
for n,m in ((10,2), (20,4), (50,10)):
    out_dir = "data/s_cvrptw_n{}m{}".format(n,m)
    os.makedirs(out_dir, exist_ok = True)

    data = VRPTW_Dataset.generate(BATCH_SIZE, n, m, tw_ratio = [0.7,0.8,1.0])

    data.normalize()
    torch.save(data, os.path.join(out_dir, "norm_data.pyth"))


# SD-CVRPTW Data
for n,m in ((10,2), (20,4), (50,10)):
    out_dir = "data/sd_cvrptw_n{}m{}".format(n,m)
    os.makedirs(out_dir, exist_ok = True)

    data = SDVRPTW_Dataset.generate(BATCH_SIZE, n, m)
    ort_routes = ort_solve(data)

    data.normalize()
    env = VRPTW_Environment(data)
    ort_costs = eval_apriori_routes(env, ort_routes, 1)

    torch.save(data, os.path.join(out_dir, "norm_data.pyth"))
    torch.save({
        "costs": ort_costs,
        "routes": ort_routes,
        }, os.path.join(out_dir, "ort.pyth"))